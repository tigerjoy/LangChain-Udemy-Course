{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Completion models - DEPRECATED - we will just use ChatModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|channel|>analysis<|message|>The user says \"Tell me a joke\". We need to comply with policy. It\\'s a request for content that is allowed. There\\'s no disallowed content. So we can provide a joke. Just give a short, clean joke. Ensure it\\'s not hateful or harassing. It\\'s fine.\\n\\nWe should respond with a joke. No policy issues.<|end|><|start|>assistant<|channel|>final<|message|>Why don‚Äôt scientists trust atoms?  \\n\\nBecause they make up everything!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI is a TextCompletion model\n",
    "# Whereas ChatOpenAI is a chat model\n",
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from pydantic import SecretStr\n",
    "\n",
    "llm = OpenAI(\n",
    "  base_url=os.getenv(\"API_URL\", \"\"),\n",
    "  api_key=SecretStr(os.getenv(\"API_KEY\", \"\")),\n",
    "  model=os.getenv(\"MODEL\", \"\"),\n",
    "  temperature=0\n",
    ")\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|channel|>analysis<|message|>The user says \"Tell me a joke\". We need to comply with policy. It\\'s a request for content that is allowed. There\\'s no disallowed content. So we can provide a joke. Just give a short, clean joke. Ensure it\\'s not hateful or harassing. It\\'s fine.\\n\\nWe should respond with a joke. No policy issues.<|end|><|start|>assistant<|channel|>final<|message|>Why don‚Äôt scientists trust atoms?  \\n\\nBecause they make up everything!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the tomato turn red? Because it saw the salad dressing!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(input=\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Processing - Make multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='<|channel|>analysis<|message|>The user wants two jokes: one about cows and one about parrots. They want \"tell me a joke about cows\" and \"tell me a joke about parrots\". So we need to provide a joke for each. The user didn\\'t specify any constraints beyond that. We should produce short, humorous jokes.\\n\\nWe must ensure the content is appropriate. Jokes about animals are fine. Provide two separate jokes. Possibly format them as bullet points or separate lines. Keep it concise and clear. No disallowed content. It\\'s straightforward.\\n\\nWe can do something like:\\n\\n- Cow joke: \"Why did the cow go to space? Because it wanted to see the Milky Way!\"\\n\\n- Parrot joke: \"What does a parrot say when it\\'s tired? \\'Polly-gone!\\'\"\\n\\nBut we need to be careful with jokes that might be offensive. These are fine.\\n\\nWe can also do:\\n\\nCow: \"Why did the cow cross the road? To get to the udder side.\"\\n\\nParrot: \"Why don\\'t parrots like to play cards? Because they always talk about their \\'bird\\' deals.\" Eh.\\n\\nBetter to keep them simple and punny.\\n\\nLet\\'s produce two jokes.<|end|><|start|>assistant<|channel|>final<|message|>**Cow joke:**  \\nWhy did', generation_info={'finish_reason': 'length', 'logprobs': None})], []] llm_output={'token_usage': {'total_tokens': 333, 'prompt_tokens': 83, 'completion_tokens': 250}, 'model_name': 'unsloth/gpt-oss-20b'} run=[RunInfo(run_id=UUID('2a26961f-9b47-43a5-b6c7-c86c6cbbb52d')), RunInfo(run_id=UUID('73122a26-515d-403b-816e-cd3c964c713a'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "result = llm.generate([\"Tell me a joke about cows\", \"Tell me a joke about parrots\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 333,\n",
       "  'prompt_tokens': 83,\n",
       "  'completion_tokens': 250},\n",
       " 'model_name': 'unsloth/gpt-oss-20b'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a conversation with SystemMessage, AIMessage and SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "  base_url=os.getenv(\"API_URL\", \"\"),\n",
    "  api_key=SecretStr(os.getenv(\"API_KEY\", \"\")),\n",
    "  model=os.getenv(\"MODEL\", \"\"),\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure, here‚Äôs one for you:\\n\\n**Why don‚Äôt cows ever get lost?**\\n\\nBecause they always follow the *moo*ving directions! üêÑüòÑ' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 73, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/gpt-oss-20b', 'system_fingerprint': 'unsloth/gpt-oss-20b', 'id': 'chatcmpl-fddg23cxnmdattu509dmr', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--5f00b388-d011-402b-8c90-996cb7e47849-0' usage_metadata={'input_tokens': 73, 'output_tokens': 76, 'total_tokens': 149, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Tell me a joke about cows\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant specialized in providing information about BellaVista Italian Restaurant.\"),\n",
    "    HumanMessage(content=\"What's on the menu?\"),\n",
    "    AIMessage(content=\"BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood.\"),\n",
    "    HumanMessage(content=\"Do you have vegan options?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes! BellaVista has several plant‚Äëbased options you can enjoy:\\n\\n| Dish | Description |\\n|------|-------------|\\n| **Vegan Margherita Pizza** | Classic tomato sauce, fresh basil, and a dairy‚Äëfree cheese blend (or no cheese at all). |\\n| **Pasta Primavera** | Whole‚Äëgrain spaghetti tossed with seasonal veggies, garlic, olive oil, and a sprinkle of nutritional yeast for a cheesy flavor. |\\n| **Vegan Mushroom Risotto** | Creamy arborio rice cooked in vegetable broth with saut√©ed mushrooms, onions, and fresh herbs. |\\n| **Caesar Salad (vegan)** | Crisp romaine, homemade croutons, lemon‚Äëolive oil dressing, and a drizzle of tahini for the ‚Äúcheese‚Äù flavor. |\\n| **Bruschetta Trio** | Classic tomato & basil, roasted red pepper, or saut√©ed spinach with garlic ‚Äì all served on toasted ciabatta. |\\n\\nIf you have any dietary restrictions or want to double‚Äëcheck ingredients (e.g., hidden dairy in sauces), just let the server know‚Äîthey‚Äôre happy to accommodate and can suggest additional vegan-friendly sides like roasted vegetables or a fresh fruit plate.\\n\\nEnjoy your meal!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 350, 'prompt_tokens': 126, 'total_tokens': 476, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/gpt-oss-20b', 'system_fingerprint': 'unsloth/gpt-oss-20b', 'id': 'chatcmpl-u6pjo0ovnuds81xteqf2o', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a85bbcaf-d875-4ad4-bfed-696d5ab36e4c-0', usage_metadata={'input_tokens': 126, 'output_tokens': 350, 'total_tokens': 476, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.invoke(input=messages)\n",
    "llm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Processing for Chat Models - Make multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='\"Gibt es vegane Optionen?\"', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='\"Gibt es vegane Optionen?\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1407, 'prompt_tokens': 90, 'total_tokens': 1497, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/gpt-oss-20b', 'system_fingerprint': 'unsloth/gpt-oss-20b', 'id': 'chatcmpl-os47mwor1x7rqbvcadmx6', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6109f5cc-3bb0-44cb-8858-8cea8e1c9af8-0', usage_metadata={'input_tokens': 90, 'output_tokens': 1407, 'total_tokens': 1497, 'input_token_details': {}, 'output_token_details': {}}))], [ChatGeneration(text='¬øTienes opciones veganas?', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='¬øTienes opciones veganas?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 92, 'total_tokens': 219, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'unsloth/gpt-oss-20b', 'system_fingerprint': 'unsloth/gpt-oss-20b', 'id': 'chatcmpl-lopr0liowx4q62nf354rq', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--f176c360-97d5-418a-a838-cddda37ca7c8-0', usage_metadata={'input_tokens': 92, 'output_tokens': 127, 'total_tokens': 219, 'input_token_details': {}, 'output_token_details': {}}))]], llm_output={'token_usage': {'completion_tokens': 1534, 'prompt_tokens': 182, 'total_tokens': 1716}, 'model_name': 'unsloth/gpt-oss-20b', 'system_fingerprint': 'unsloth/gpt-oss-20b'}, run=[RunInfo(run_id=UUID('6109f5cc-3bb0-44cb-8858-8cea8e1c9af8')), RunInfo(run_id=UUID('f176c360-97d5-418a-a838-cddda37ca7c8'))], type='LLMResult')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to German\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates the English to Spanish.\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "]\n",
    "batch_result = llm.generate(batch_messages)\n",
    "batch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the output ourself (we will later take a look at Output-Parsers!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Gibt es vegane Optionen?\"', '¬øTienes opciones veganas?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = [generation[0].text for generation in batch_result.generations]\n",
    "translations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
