{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_3232\\3518869110.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'a parrot',\n",
       " 'text': 'Why did the parrot bring a ladder to the bar?\\n\\nBecause it heard the drinks were on the house! ü¶úüçπ'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "from pydantic import SecretStr\n",
    "\n",
    "llm = ChatOpenAI( \n",
    "  model=os.getenv(\"MODEL\", \"\"),\n",
    "  base_url=os.getenv(\"API_URL\", \"\"),\n",
    "  api_key=SecretStr(os.getenv(\"API_KEY\", \"\")),\n",
    "  temperature=0\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"input\"], template=\"Tell me a joke about {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.invoke(input={\"input\": \"a parrot\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'a parrot',\n",
       " 'language': 'Bengali',\n",
       " 'text': '‡¶è‡¶ï‡¶ü‡¶æ ‡¶™‡¶æ‡¶ñ‡¶ø (‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶ü) ‡¶ó‡¶æ‡¶õ‡ßá‡¶∞ ‡¶°‡¶æ‡¶≤‡ßá ‡¶¨‡¶∏‡ßá ‡¶õ‡¶ø‡¶≤‡•§  \\n‡¶Ö‡¶ö‡ßá‡¶®‡¶æ ‡¶≤‡ßã‡¶ï ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡ßá‡¶∏ ‡¶ï‡¶∞‡¶≤, ‚Äú‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßÄ ‡¶¨‡¶≤‡ßã?‚Äù  \\n\\n‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶ü ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶≤, ‚Äú‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã ‚Äò‡¶ï‡ßÄ‚Äô ‡¶¨‡¶≤‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø ‡¶®‡¶æ, ‡¶Ü‡¶Æ‡¶ø ‡¶§‡ßã ‚Äò‡¶ï‡ßÄ-‡¶ì‡ßü‡¶æ‡¶á‚Äô!‚Äù'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables=[\"input\", \"language\"], template=\"Tell me a joke about {input} in {language}. Don't think too much, just say the first joke that comes to mind.\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.invoke({\"input\": \"a parrot\", \"language\": \"Bengali\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.sequential import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a review given a dish name and the experience.\n",
    "prompt_review = PromptTemplate.from_template(\n",
    "    template=\"You ordered {dish_name} and your experience was {experience}. Write a review: \"\n",
    ")\n",
    "chain_review = LLMChain(llm=llm, prompt=prompt_review, output_key=\"review\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up comment given the restaurant review.\n",
    "prompt_comment = PromptTemplate.from_template(\n",
    "    template=\"Given the restaurant review: {review}, write a follow-up comment: \"\n",
    ")\n",
    "chain_comment = LLMChain(llm=llm, prompt=prompt_comment, output_key=\"comment\")\n",
    "\n",
    "# This is an LLMChain to summarize a review.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"Summarise the review in one short sentence: \\n\\n {comment}\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "# This is an LLMChain to translate a summary into German.\n",
    "prompt_translation = PromptTemplate.from_template(\n",
    "    template=\"Translate the summary to Bengali: \\n\\n {summary}\"\n",
    ")\n",
    "chain_translation = LLMChain(\n",
    "    llm=llm, prompt=prompt_translation, output_key=\"bengali_translation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'Pizza Salami',\n",
       " 'experience': 'It was awful!',\n",
       " 'review': '**Title:** A Disappointing Slice of Disaster\\n\\nI‚Äôve been a loyal fan of this pizzeria for years, but my recent order of the Pizza Salami has left me utterly disappointed‚Äîand I‚Äôm not just talking about the taste.\\n\\n**What Went Wrong**\\n\\n- **Toppings:** The salami was thin, dry, and oddly bland. It seemed like it had been sitting in the fridge for days before being placed on the pizza.\\n- **Cheese:** Instead of a smooth, melty layer, the cheese was rubbery and stuck to the crust, making every bite feel more like chewing than eating.\\n- **Crust:** The base was soggy in the middle and burnt at the edges. It didn‚Äôt have that satisfying crunch I‚Äôve come to expect from their hand‚Äëtossed dough.\\n- **Overall Flavor:** There was a faint metallic aftertaste‚Äîlikely from the overcooked salami‚Äîthat clung to my palate long after the last bite.\\n\\n**Service & Delivery**\\n\\nThe pizza arrived 30 minutes late, and when it finally did, it was lukewarm. The delivery driver apologized but didn‚Äôt offer any compensation or follow‚Äëup. I called customer service, only to be told that ‚Äúeverything is fine‚Äù and that they couldn‚Äôt do anything about a single order.\\n\\n**Bottom Line**\\n\\nI‚Äôve never had such a disappointing experience with this place. If you‚Äôre looking for a satisfying pizza night, steer clear of the Pizza Salami‚Äîat least until they fix their cooking process and quality control. I‚Äôll be waiting to see if future orders improve, but right now, it‚Äôs a hard no from me.',\n",
       " 'comment': '**Response from the Owner**\\n\\nHi there,\\n\\nThank you for taking the time to share your experience‚Äîand I‚Äôm truly sorry it fell so far short of what we strive for at [Pizzeria Name]. Your feedback is exactly what we need to keep improving.\\n\\nI‚Äôve already spoken with our kitchen team about the salami and cheese issues, and we‚Äôre tightening our quality‚Äëcontrol checks. We‚Äôre also reviewing our delivery times and training our drivers on how to handle late or lukewarm orders‚Äîso this won‚Äôt happen again.\\n\\nTo make things right, I‚Äôd like to offer you a **free Pizza Salami** (or your choice of any pizza) plus a 20\\u202f% discount on your next order. Please DM me your best contact details and we‚Äôll arrange delivery at no cost to you.\\n\\nAgain, I apologize for the disappointment and appreciate your patience as we work to get back to the standard our loyal customers deserve. We hope to earn back your trust soon.\\n\\nWarm regards,\\n\\n[Owner‚Äôs Name]  \\nFounder & Head Chef, [Pizzeria Name]',\n",
       " 'summary': 'Owner apologizes, fixes quality and delivery problems, and offers a free pizza plus 20\\u202f% off next order to win back the customer‚Äôs trust.',\n",
       " 'bengali_translation': '‡¶Æ‡¶æ‡¶≤‡¶ø‡¶ï ‡¶ï‡ßç‡¶∑‡¶Æ‡¶æ ‡¶ö‡ßá‡ßü‡ßá ‡¶ó‡ßÅ‡¶£‡¶ó‡¶§ ‡¶Æ‡¶æ‡¶® ‡¶ì ‡¶°‡ßá‡¶≤‡¶ø‡¶≠‡¶æ‡¶∞‡¶ø ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡ßá‡¶® ‡¶è‡¶¨‡¶Ç ‡¶ó‡ßç‡¶∞‡¶æ‡¶π‡¶ï‡ßá‡¶∞ ‡¶Ü‡¶∏‡ßç‡¶•‡¶æ ‡¶™‡ßÅ‡¶®‡¶∞‡ßÅ‡¶¶‡ßç‡¶ß‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡ßç‡¶∞‡¶ø ‡¶™‡¶ø‡¶ú‡ßç‡¶ú‡¶æ ‡¶ì ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞‡ßá ‡ß®‡ß¶\\u202f% ‡¶õ‡¶æ‡¶°‡¶º‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡¶æ‡¶¨ ‡¶¶‡ßá‡¶®‡•§'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_review, chain_comment, chain_summary, chain_translation],\n",
    "    input_variables=[\"dish_name\", \"experience\"],\n",
    "    output_variables=[\"review\", \"comment\", \"summary\", \"bengali_translation\"],\n",
    ")\n",
    "\n",
    "overall_chain.invoke({\"dish_name\": \"Pizza Salami\", \"experience\": \"It was awful!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for analyzing neutral sentiments\",\n",
    "        \"prompt_template\": neutral_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B8F7CBB4D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B8F8174050>, root_client=<openai.OpenAI object at 0x000001B8F7CB8EC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B8F7CBBCB0>, model_name='openai/gpt-oss-20b', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://127.0.0.1:1234/v1'), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'neutral': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B8F7CBB4D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B8F8174050>, root_client=<openai.OpenAI object at 0x000001B8F7CB8EC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B8F7CBBCB0>, model_name='openai/gpt-oss-20b', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://127.0.0.1:1234/v1'), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'negative': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001B8F7CBB4D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001B8F8174050>, root_client=<openai.OpenAI object at 0x000001B8F7CB8EC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001B8F7CBBCB0>, model_name='openai/gpt-oss-20b', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://127.0.0.1:1234/v1'), output_parser=StrOutputParser(), llm_kwargs={})}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: Good for analyzing positive sentiments\n",
      "neutral: Good for analyzing neutral sentiments\n",
      "negative: Good for analyzing negative sentiments\n"
     ]
    }
   ],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} output_parser=RouterOutputParser() partial_variables={} template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\npositive: Good for analyzing positive sentiments\\nneutral: Good for analyzing neutral sentiments\\nnegative: Good for analyzing negative sentiments\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (must include ```json at the start of the response) >>\\n<< OUTPUT (must end with ```) >>\\n'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_3232\\1654321065.py:11: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: {'input': 'I ordered Pizza Salami for 9.99$ and it was awesome!'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I ordered Pizza Salami for 9.99$ and it was awesome!',\n",
       " 'text': '**Positive Highlights**\\n\\n- **Great Value:** The pizza cost only $9.99‚Äîan affordable price for a tasty meal.\\n- **Delicious Taste:** You described the pizza as ‚Äúawesome,‚Äù indicating it met or exceeded your expectations in flavor and quality.\\n- **Satisfaction:** Your overall experience was positive, showing that the order delivered enjoyment and satisfaction.\\n\\nIt sounds like you had a wonderful, budget‚Äëfriendly pizza night!'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "print(router_prompt)\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=destination_chains[\"neutral\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"I ordered Pizza Salami for 9.99$ and it was awesome!\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
